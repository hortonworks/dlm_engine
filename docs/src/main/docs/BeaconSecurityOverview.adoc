//<!--
// Copyright  (c) 2016-2017, Hortonworks Inc.  All rights reserved.
//
// Except as expressly permitted in a written agreement between you or your
// company and Hortonworks, Inc. or an authorized affiliate or partner
// thereof, any use, reproduction, modification, redistribution, sharing,
// lending or other exploitation of all or any part of the contents of this
// software is strictly prohibited.
//-->

= Beacon Security Overview


== Introduction

Beacon is the Hadoop engine for the data replication, backups, tiering  and disaster recovery.    It will be delivered in a few phases.   Phase 1 works with the HDP stack component features available as part of Fenton and has dependencies on the following components.

* HDFS
* Hive

The following are not for V1

* Ranger
* Atlas


|===
|*Phase 1 Goal*|Goal of Phase 1 is to provide data replication and basic DR features that take advantage of Fenton enhancements in Hive and HDFS [line-through]#along with Ranger and Atlas policy handling for datasets# *(Not for V1)*.   This document specifically talks about the security aspects of Beacon.

|===


== High level overview


Beacon will  provide the following security features:

* Support for authentication based on Kerberos
* Support for Kerberos enabled stack components that Beacon uses
* Support for authorization for various operations on the entities
* Superuser Configuration
* Configuration of Admin users and groups
* Support for credential provider alias for passwords.
* Group Memberships
* Beacon privileges
* Knox SSO support
* Authorization Policy
* Ports

The following features are moved out of V1

* Support for Proxying Users
* TLS support for stack components (rpc protection level of integrity)
* TLS support for Beacon endpoint (rpc protection level of integrity)
* TDE support
* Ranger and Atlas integration

== Support for authentication based on Kerberos

Beacon supports two authentication methods, simple and kerberos.   The simple/(pseudo) scheme authenticates  the user by simply trusting the value of the query string parameter 'user.name' for REST purposes.   In Kerberos authentication, beacon uses HTTP Kerberos SPNEGO to authenticate the user. +
Beacon will be configured as a Hadoop service component with a principal name beacon/HOST@REALM with a random password.      A keytab will be generated and used by Beacon to login to the kerberos environment.   All beacon REST endpoints will be protected  and only authenticated users will be allowed access.    Once authentication has been established beacon will set a signed HTTP Cookie that contains an authentication token with the user name, user principal, authentication type and expiration time.  This is done using the standard Hadoop Auth facility.

== Support for Kerberos enabled stack components

Beacon interacts with Hadoop components as part of the replication work.   In a typical production environment, the hadoop cluster will be kerberos-enabled and all critical services will be kerberos protected services.   Beacon itself will be configured to login to the kerberos environment and establish its identity so that hadoop services can allow beacon to access the services.    All the configuration settings for different services will be used to access the services (principals etc)


== Support for authorization for various operations on the entities

Beacon has entities describing clusters and policies.   Operation on these entities (creation, deletion, submit and schedule) are controlled by policies.   The default authorizer uses Hadoop group mapping and membership services to restrict access to entities.

== Superuser configuration

Beacon has the concept of super-user which is the configuration less identity - it is simply the user that runs the  beacon process..Typically it will be the user _beacon_.    Additionally, beacon configuration provides a group to be identified as a super user group whose members are granted superuser privileges. +
*A superuser has access to all resources (admin or otherwise) hosted by beacon*

== Admin Users and Groups

In addition to the superuser, Beacon allows list of users to be identified as admin users (those with access to admin resources).   Similar to supergroup, a list of groups can be also identified as admin groups.   An admin user has access to the admin APIs and can perform all entity operations.   Currently admin and superuser groups and membership are no different (except the default superuser is identified by process ownership) but the distinction can become more in later releases.

== Credential Provider Alias for Passwords

Beacon uses the Hadoop credential provider service to store sensitive information like passwords

== Group Memberships

The list of groups of an authenticated user is determined by a group mapping service, configured in the hadoop cluster. The default implementation maps to  the OS groups the authenticated user belongs to.

== Beacon privileges


As the subject that performs replication, backup and tiering, the beacon user needs to have access to move all data from one cluster to another, retain original  permissions etc, run jobs on behalf of various users.   This requires beacon user to be configured as a Hadoop superuser.   When beacon runs the job, it will use the user identified during policy submission as the user who will move the data.   This means that the user has the ability to create snapshots and access the data underneath irrespective of the ownership of the data.

Typically an HDFS superuser has these privileges across all the filesystem and the default user will be hdfs if no user is explicitly identified.  It is expected that user used to run the job is either member of the superuser group of HDFS or has sufficient privileges to all data under the folder used for replication. Similarly Hive jobs will run as the provided user and the default being hive user.

== Knox SSO Support

While Beacon does not support negotiating Knox protected endpoints for its internal communications between clusters and replication, Beacon APIs can be potentially protected by Knox and can participate in Knox SSO.  [line-through]#This is not currently the target of V1#.  Because of Dataplane requirements this is a must have and is now port of V1.

== Authorization Policy

Cluster entities can be added/deleted only by superuser, members of the superusers group, admin users and members of admin groups +
Policy entities  and policy  Instance operations are allowed by owners (creators of the policy) and members of admin and superuser groups. +
Beacon exposes a set of administrative resources (configuration information, diagnostics etc).   These can be accessed by super user, one of the admin users or members of either superuser and admin groups.

== Ports

The default port for Beacon will be 25968 (for non-TLS) and 25443 for TLS enabled endpoints)


== Support for Proxying Users
*Not for V1*

Beacon replication policies can be scheduled on behalf of different users.   To allow this, beacon will be registered as a trusted super user in hadoop with explicit restriction that only users belonging to specific groups will be proxied to submit users.   The groups are TBD (potentially can be the dataplane user group)


== TLS support for stack components (rpc protection level of integrity)
*Not for V1*

Beacon will allow wire encryption to be enabled to service endpoints (HDFS, Hive endpoints).   The necessary certificates will need to be imported to the trust store used by beacon

== TLS support for Beacon endpoint
*Not for V1*

Beacon can be configured to use a TLS protected endpoint.   Only TLS 1.2 protocol will be supported.

== TDE support
*Not for V1*

Beacon supports moving data between TDE protected zones as well as between TDE and non TDE zones.   The zones can have the same or different encryption zone keys  even though for DR use cases it will be typically different zones.   In the case of differing keys, the client copying the data will decrypt and reencrypt the data as needed (this is a  variation of copying between TDE and non TDE zones).

When TDE support is desired, beacon should be configured as a proxy super user in KMS.

== Ranger and Atlas Integration
*Not for V1*


Apart from the ranger authorization for beacon entities and resources as described above, beacon will copy ranger policies associated with a dataset as part of replication.   The order of copying would be

 * Ranger policies
 * Atlas policies, lineage, tags
 * Dataset (Hive DB or HDFS folder)

When exporting and importing Ranger policies, ranger plugin will have the intelligence to only export and import the policies as well as protect the dataset appropriately for non beacon users. +
Similarly Atlas tags and lineage would be imported and updates to them will also be protected by the Atlas plugin (potentially having additional policies enforced by Ranger)

